{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import unittest\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0\n",
    "There are 10 images on the page http://newmantaylor.com/gallery.html. Some of them have \"alt text\", which is the text that is displayed or spoken because of browser limitations, or because someone is using a screen reader, for example. Scrape this page and print out the alt text for each image. If there is no alt text, print \"No alternative text provided!\" The code you write should be general enough to work for any similar page with 10 images like this (not just this one), just by changing the URL to a different one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waving Kitty 1\n",
      "No alternative text provided!\n",
      "Waving Kitty 3\n",
      "Waving Kitty 4\n",
      "Waving Kitty 5\n",
      "Waving Kitty 6\n",
      "No alternative text provided!\n",
      "Waving Kitty 8\n",
      "Waving Kitty 9\n",
      "Waving Kitty 10\n"
     ]
    }
   ],
   "source": [
    "page = requests.get(\"http://newmantaylor.com/gallery.html\")\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "img_tags = soup.find_all(\"img\")\n",
    "for img in img_tags:\n",
    "    print(img.get(\"alt\", \"No alternative text provided!\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "Access and cache data, starting from https://www.nps.gov/index.htm. You will ultimately need the HTML data from all the parks from Arkansas, California, and Michigan. So, you should save on your computer data from the following pages, in files with the following names:\n",
    "\n",
    "Main page data, https://www.nps.gov/index.htm, in a file nps_gov_data.html\n",
    "\n",
    "Arkansas, https://www.nps.gov/state/ar/index.htm, in a file arkansas_data.html\n",
    "\n",
    "California, https://www.nps.gov/state/ca/index.htm, in a file california_data.html\n",
    "\n",
    "Michigan, https://www.nps.gov/state/mi/index.htm, in a file michigan_data.html\n",
    "\n",
    "You should commit and push each of these .html files to your final Git repository.\n",
    "\n",
    "Note that this is a much less complex 'caching' system to save data from the internet on your computer than you may be accustomed to from accessing REST API data. A system like the one discussed in our textbook can also certainly be used for HTML data like this, but in this case, we're using a shortcut. Later in the course you'll see more options for structuring your code in an easily-reusable way, and you may also come up with some yourself.\n",
    "\n",
    "You should not hardcode the above URLs to get the html. Instead, you should write code that begins scraping https://www.nps.gov/index.htm. This code should be written such that you can pretty easily decide to add a new state, such as NY, to the states you want data from, and it would work.\n",
    "\n",
    "You can access, and thus cache, data from those three listed pages for AK, CA, and MI parks, by starting with a BeautifulSoup object of the HTML on the page https://www.nps.gov/index.htm -- and to get full points on this question, you should do that, rather than simply e.g. resp_text = requests.get(\"https://www.nps.gov/state/mi/index.htm\").text, etc.\n",
    "\n",
    "If you access and cache the data without starting with a BeautifulSoup instance from the https://www.nps.gov/index.htm data, you will lose at least 50 points from this problem.\n",
    "\n",
    "We have provided comments as structure to proceed through Part 1. We suggest that you follow them. However, if you choose not to, as long as you end up with the result that is required (the files), and you are not accessing the internet to get HTML data from those 4 pages every time you run the program, you will get credit for Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# found this to be easier to get as many states as you want. Just need to add abbreviates to the 'states' list\n",
    "# you can then accumulate them all in a list comprehension to get a list of each page's html\n",
    "def get_and_cache_page(relative_url, filename):\n",
    "    base_url = \"https://www.nps.gov\"\n",
    "    try:\n",
    "        page = open(filename, 'r').text\n",
    "    except:\n",
    "        page = requests.get(base_url + relative_url).text\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(page)\n",
    "    return page\n",
    "\n",
    "def get_state_url(state_abrs):\n",
    "    if not isinstance(state_abrs, list):\n",
    "        state_abrs = list(state_abrs)\n",
    "    state_urls = [main_soup.find('a', href=True, text=item)['href'] for item in states]\n",
    "    return state_urls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "nps_main = get_and_cache_page(\"/index.htm\", \"nps_gov_data.html\")\n",
    "main_soup = BeautifulSoup(nps_main, 'html.parser')\n",
    "\n",
    "\n",
    "states = ['Arkansas', 'California', 'Michigan']\n",
    "urls = get_state_url(states)\n",
    "\n",
    "nps_ar = get_and_cache_page(urls[0], \"arkansas_data.html\")\n",
    "nps_ca = get_and_cache_page(urls[1], \"california_data.html\")\n",
    "nps_mi = get_and_cache_page(urls[2], \"michigan_data.html\")\n",
    "\n",
    "ar_soup = BeautifulSoup(nps_ar, 'html.parser')\n",
    "ca_soup = BeautifulSoup(nps_ca, 'html.parser')\n",
    "mi_soup = BeautifulSoup(nps_mi, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "Define a class NationalSite that accepts a BeautifulSoup object as input to its constructor, representing 1 National Park / National Lakeshore / etc (e.g. what you see here or what you see here)\n",
    "\n",
    "A NationalSite instance should have the following instance variables:\n",
    "\n",
    "location (state, or a city, or states ... whatever location description is provided)\n",
    "name (e.g. \"Alcatraz Island\", \"Channel Islands\"...)\n",
    "type (e.g. \"National Lakeshore\", \"National Monument\"... if there is no specified type, this value should be the special value None)\n",
    "description (e.g. \"Established in 1911 by presidential proclamation, Devils Postpile National Monument protects and preserves the Devils Postpile formation, the 101-foot high Rainbow Falls, and pristine mountain scenery. The formation is a rare sight in the geologic world and ranks as one of the world's finest examples of columnar basalt. Its columns tower 60 feet high and display an unusual symmetry.\" -- if there is no description, this instance variable should have the value of the empty string, \"\")\n",
    "A NationalSite instance should also have the following methods:\n",
    "\n",
    "A string method __str__ that returns a string of the format National Park/Site/Monument Name | Location\n",
    "\n",
    "A get_mailing_address method that returns a string representing the mailing address of the park/site/etc. Because a multi-line string will make a CSV more difficult, you should separate the lines in the address with a forward slash, like this: /. However you decide to get this information and relatively-sensibly put it together is fine. In fact, some addresses may have information included in them twice, e.g. \"Yosemite National Park, CA 95389 / Yosemite National Park / CA / 95389\", while some will not -- that is also OK! There is enough information to send mail if possible there, which is all that matters for our purposes: is there some address info that will be returned in a single-line string from this function? If so, that is success.\n",
    "\n",
    "HINT: This address info can be found by clicking the Basic Information link that each park/site/monument specification has; even parks that have many locations have a specific mailing address. It looks like this for Old Spanish in California.\n",
    "\n",
    "NOTE: If a park has no mailing address, the return value of this function should be the empty string (\"\").\n",
    "\n",
    "A __contains__ method that checks whether the additional input to the method is included in the string of the park's name. If the input is inside the name of the park, this method should return True; otherwise, it should return False.\n",
    "\n",
    "Note that you may make additional design decisions when you define your class NationalSite to help you write these methods successfully -- e.g. you could add other instance variables or other methods if you wanted to/found them useful.\n",
    "\n",
    "After you complete this, you should try creating an instance of your NationalSite class with the following code, to test and see if your class definition worked properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of what we're working w/\n",
    "alca = ca_soup.find('li', class_='clearfix')\n",
    "# print(type(alca), alca.prettify())\n",
    "binfo = [link['href'] for link in alca.find_all('a', href=True) if \"Basic Information\" in link.text][0]\n",
    "info_soup = BeautifulSoup(requests.get(binfo).text, 'html.parser')\n",
    "# info_soup.find(\"div\", class_=\"mailing-address\")\n",
    "street = info_soup.select('.street-address')[0].text\n",
    "# street.strip().replace('\\n', '/')\n",
    "# street.replace(\"\\n\", \"/\")\n",
    "# ' '.join([info_soup.find('span', {\"itemprop\": \"addressLocality\"}).text, info_soup.find('span', {\"itemprop\": \"addressRegion\"}).text, info_soup.find('span', {\"itemprop\": \"postalCode\"}).text])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NationalSite(object):\n",
    "    def __init__(self, site_soup):\n",
    "        self.location = site_soup.find('h4').text\n",
    "        self.name = site_soup.find('h3').text\n",
    "        try:\n",
    "            self.description = site_soup.find('p').text\n",
    "        except:\n",
    "            self.description = ''\n",
    "        if site_soup.find('h2').text == '':\n",
    "            self.type = None\n",
    "        else:\n",
    "            self.type = site_soup.find('h2').text\n",
    "        self.soup = site_soup\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"{0} | {1}\".format(self.name, self.location)\n",
    "    \n",
    "    def get_mailing_address(self):\n",
    "        # not sure if space comes before Basic Information each time so i'll play it safe\n",
    "        # info_url = self.soup.find('a', href=True, text=\"Basic Information\")\n",
    "\n",
    "        info_url = [link['href'] for link in self.soup.find_all('a', href=True) if \"Basic Information\" in link.text][0]\n",
    "        soup_info = BeautifulSoup(requests.get(info_url).text, 'html.parser')\n",
    "        try:\n",
    "            mail_address = soup_info.find(\"p\", class_=\"adr\").text.strip().replace('\\n', '/')\n",
    "            # get rid of the three '///' in a row\n",
    "            mail_address = mail_address.replace(re.findall(r'[/]{3}', mail_address)[0], '/')\n",
    "            return mail_address\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "    def __contains__(self, input):\n",
    "        return input in self.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "Create a list of NationalSite objects from each one of these 3 states: Arkansas, California, and Michigan. They should be saved in the following variables, respectively:\n",
    "\n",
    "- arkansas_natl_sites\n",
    "- california_natl_sites\n",
    "- michigan_natl_sites\n",
    "(You may accumulate these lists in any way you prefer.)\n",
    "\n",
    "Write 3 CSV files, arkansas.csv, california.csv, michigan.csv -- one for each state's national parks/sites/etc, each of which has 5 columns:\n",
    "\n",
    "- Name\n",
    "- Location\n",
    "- Type\n",
    "- Address\n",
    "- Description\n",
    "- Remember to handle e.g commas and multi-line strings so that data for 1 field all ends up inside 1 spreadsheet cell when you open the CSV!\n",
    "\n",
    "For any park/site/monument/etc where a value is None, you should put the string \"None\" in the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arkansas_natl_sites = [NationalSite(x) for x in ar_soup.find_all('li', class_='clearfix', id=True)]\n",
    "california_natl_sites = [NationalSite(x) for x in ca_soup.find_all('li', class_='clearfix', id=True)]\n",
    "michigan_natl_sites = [NationalSite(x) for x in mi_soup.find_all('li', class_='clearfix', id=True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def write_to_csv(filename, site_list):\n",
    "    with open(filename, 'w') as outfile:\n",
    "        outwriter = csv.writer(outfile, delimiter=',')\n",
    "        header = [\"Name\", \"Location\", \"Type\", \"Address\", \"Description\"]\n",
    "        outwriter.writerow(header)\n",
    "\n",
    "        for site in site_list:\n",
    "            if site.type is None:\n",
    "                typ = \"None\"\n",
    "            else:\n",
    "                typ = site.type\n",
    "            row = [site.name, site.location, typ, site.get_mailing_address(), site.description]\n",
    "            outwriter.writerow(row)\n",
    "\n",
    "\n",
    "write_to_csv(\"arkansas.csv\", arkansas_natl_sites)\n",
    "write_to_csv(\"california.csv\", california_natl_sites)\n",
    "write_to_csv(\"michigan.csv\", michigan_natl_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
